{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32b39d-aac6-454b-9d9a-2fa494ac5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/atom/Dropbox/Skule/EECE571F/Project/research-contributions/UNETR/BTCV/networks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde958e-cf32-480f-b2d8-1e2a79baf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from unetr import UNETR\n",
    "# from self_attention_cv.Transformer3Dsegmentation.tranf3Dseg import Transformer3dSeg\n",
    "# from self_attention_cv.transunet import TransUnet\n",
    "# from self_attention_cv import TransformerEncoder\n",
    "# from self_attention_cv import ViT, ResNet50ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624650bb-cc6a-413a-ba42-4083109d4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs: torch.Tensor, labels: torch.Tensor, smooth=1e-6):\n",
    "    # multiclass IOU: intersection is where classes agree; union is any non-null class\n",
    "    \n",
    "    # output shape is (8, 4, 3, 3, 3), one segment for each patch\n",
    "    outputs = outputs.argmax(axis=1)\n",
    "    labels = labels.argmax(axis=1)\n",
    "    \n",
    "    intersection = (outputs == labels).float().sum((-1, -2, -3)) # Will be zero if Truth=0 or Prediction=0\n",
    "    union = ((outputs != 0) | (labels != 0)).float().sum((-1, -2, -3)) # Will be zero if both are 0\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth) # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    return thresholded.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ef12d-22d0-424d-bb99-c3106853f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1e-6):\n",
    "        # flatten label and prediction tensors\n",
    "        inputs = torch.flatten(inputs)\n",
    "        targets = torch.flatten(targets)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2 * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f3815-8ff6-493b-9b18-e1608e1cf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.modules.loss._WeightedLoss):\n",
    "    \"\"\"\n",
    "    From https://github.com/ZFTurbo/segmentation_models_3D/blob/cc9f4fdd22387cc1556c77a85c7bea43e541ef1d/segmentation_models_3D/base/functional.py#L259\n",
    "    as oppose to https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py#L11\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, gamma=2, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.weight = weight # weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, pr, gt, smooth=1e-6):\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pr = torch.clamp(pr, smooth, 1 - smooth)\n",
    "        \n",
    "        focal_loss = -1 * gt * torch.log(pr) * (self.alpha * (1 - pr) ** self.gamma)\n",
    "        \n",
    "        # ce_loss = torch.nn.functional.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        # pt = torch.exp(-ce_loss)\n",
    "        # focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e10e54-35c4-4978-99ce-63f923096f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='bool')[y.astype(np.int16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0b58f-7bb8-4d30-981a-604aa85af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brats_batch(prefix, \n",
    "                         contrasts, \n",
    "                         batch_size=32, \n",
    "                         tumour='*', \n",
    "                         patient_ids='*',\n",
    "                         resample_size=(None, None, None),\n",
    "                         augment_size=None,\n",
    "                         infinite=True):\n",
    "    \"\"\"\n",
    "    Generate arrays for each batch, for x (data) and y (labels), where the contrast is treated like a colour channel.\n",
    "    \n",
    "    Example:\n",
    "    x_batch shape: (32, 240, 240, 155, 4)\n",
    "    y_batch shape: (32, 240, 240, 155)\n",
    "    \n",
    "    augment_size must be less than or equal to the batch_size, if None will not augment.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_pattern = '{prefix}/MICCAI_BraTS_2018_Data_Training/{tumour}/{patient_id}/{patient_id}_{contrast}.nii.gz'\n",
    "    while True:\n",
    "        n_classes = 4\n",
    "\n",
    "        # get list of filenames for every contrast available\n",
    "        keys = dict(prefix=prefix, tumour=tumour)\n",
    "        filenames_by_contrast = {}\n",
    "        for contrast in contrasts:\n",
    "            filenames_by_contrast[contrast] = glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_ids, **keys)) if patient_ids == '*' else []\n",
    "            if patient_ids != '*':\n",
    "                contrast_files = []\n",
    "                for patient_id in patient_ids:\n",
    "                    contrast_files.extend(glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_id, **keys)))\n",
    "                filenames_by_contrast[contrast] = contrast_files\n",
    "\n",
    "        # get the shape of one 3D volume and initialize the batch lists\n",
    "        arbitrary_contrast = contrasts[0]\n",
    "        shape = nib.load(filenames_by_contrast[arbitrary_contrast][0]).get_fdata().shape if resample_size == (None, None, None) else resample_size\n",
    "\n",
    "        # initialize empty array of batches\n",
    "        x_batch = np.empty((batch_size, ) + shape + (len(contrasts), )) #, dtype=np.int32)\n",
    "        y_batch = np.empty((batch_size, ) + shape + (n_classes,)) #, dtype=np.int32)\n",
    "        num_images = len(filenames_by_contrast[arbitrary_contrast])\n",
    "        np.random.shuffle(filenames_by_contrast[arbitrary_contrast])\n",
    "        for bindex in trange(0, num_images, batch_size):\n",
    "            filenames = filenames_by_contrast[arbitrary_contrast][bindex:bindex + batch_size]\n",
    "            for findex, filename in enumerate(filenames):\n",
    "                for cindex, contrast in enumerate(contrasts):\n",
    "\n",
    "                    # load raw image batches and normalize the pixels\n",
    "                    tmp_img = nib.load(filename.replace(arbitrary_contrast, contrast)).get_fdata()\n",
    "                    if resample_size != (None, None, None):\n",
    "                        tmp_img = resize(tmp_img, resample_size, mode='edge')\n",
    "                    tmp_img = scalar.fit_transform(tmp_img.reshape(-1, tmp_img.shape[-1])).reshape(tmp_img.shape)\n",
    "                    x_batch[findex, ..., cindex] = tmp_img\n",
    "\n",
    "                    # load mask batches and change to categorical\n",
    "                    tmp_mask = nib.load(filename.replace(arbitrary_contrast, 'seg')).get_fdata()\n",
    "                    tmp_mask[tmp_mask==4] = 3\n",
    "                    tmp_mask = to_categorical(tmp_mask, num_classes=4)\n",
    "                    if resample_size != (None, None, None):\n",
    "                        tmp_mask = resize(tmp_mask, resample_size, mode='edge')\n",
    "                    y_batch[findex] = tmp_mask\n",
    "\n",
    "            if bindex + batch_size > num_images:\n",
    "                x_batch, y_batch = x_batch[:num_images - bindex], y_batch[:num_images - bindex]\n",
    "            if augment_size is not None:\n",
    "                # x_aug, y_aug = augment(x_batch, y_batch, augment_size)\n",
    "                x_aug = None\n",
    "                y_aug = None\n",
    "                yield np.append(x_batch, x_aug), np.append(y_batch, y_aug)\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "        if not infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaf9c9-f593-4d08-b620-deb8221256a3",
   "metadata": {},
   "source": [
    "Model Architecture Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c437d0-2cfa-454f-a790-0dde5a8e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/atom/Documents/datasets/brats' # Adam's Station\n",
    "output_dir = prefix + '/unetr_models/'\n",
    "batch_size = 6 # 16\n",
    "contrasts = ['t1ce', 'flair', 't2', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213fd10-539d-4aaa-9dff-951031f0ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_classes = 4\n",
    "brats_contrasts = 4\n",
    "brats_x = 80 # 240\n",
    "brats_y = 80 # 240\n",
    "brats_z = 80 # 155\n",
    "resampled_shape = (brats_x, brats_y, brats_z)\n",
    "\n",
    "patch_size = 16\n",
    "feature_size = 20\n",
    "embedding_size = 768 # 1536\n",
    "msa_heads = 6 # 12\n",
    "mlp_size = 1024 # 2048\n",
    "\n",
    "dropout = 0.10\n",
    "max_epochs = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cf408-76ea-4e88-8ee1-d978f9ec42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OURUNETR(UNETR):\n",
    "    \n",
    "    @property\n",
    "    def patch_size(self):\n",
    "        return (patch_size, patch_size, patch_size)\n",
    "    \n",
    "    @patch_size.setter\n",
    "    def patch_size(self, value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476d812-9e5f-443c-a879-efe1c46b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1ea11-8bb7-467e-8791-7cb9b11a9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OURUNETR(\n",
    "    in_channels=brats_contrasts,\n",
    "    out_channels=brats_classes,\n",
    "    img_size=(brats_x, brats_y, brats_z),\n",
    "    feature_size=feature_size,\n",
    "    hidden_size=embedding_size,\n",
    "    mlp_dim=mlp_size,\n",
    "    num_heads=msa_heads,\n",
    "    pos_embed='perceptron',\n",
    "    norm_name='instance',\n",
    "    conv_block=True,\n",
    "    res_block=True,\n",
    "    dropout_rate=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227263a-a52c-4368-9b57-07a2cd885b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "focal_loss = FocalLoss()\n",
    "# dice_loss = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663b51d-8558-4c4e-9e0a-9b859adbe851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906d198-0579-4d3a-b989-97356a3eac1f",
   "metadata": {},
   "source": [
    "Training and Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1b39f-c678-42f7-9296-f344bf13a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "brats_dir = '/MICCAI_BraTS_2018_Data_Training/'\n",
    "\n",
    "data_list_LGG = os.listdir(os.path.join(prefix+brats_dir,'LGG'))\n",
    "data_list_HGG = os.listdir(os.path.join(prefix+brats_dir,'HGG'))\n",
    "dataset_file_list = data_list_HGG + data_list_LGG\n",
    "\n",
    "# shuffle and split the dataset file list\n",
    "random.seed(42)\n",
    "file_list_shuffled = dataset_file_list.copy()\n",
    "random.shuffle(file_list_shuffled)\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_file, test_file = file_list_shuffled[0:int(len(file_list_shuffled)*(1-test_ratio))], file_list_shuffled[int(len(file_list_shuffled)*(1-test_ratio)):]\n",
    "\n",
    "while '.DS_Store' in train_file:\n",
    "    train_file.remove('.DS_Store')\n",
    "while '.DS_Store' in test_file:\n",
    "    test_file.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dda77-b81e-4f6a-a53e-b44b128e6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_segmentation(mask_block):\n",
    "    \"\"\"\n",
    "    For a given block, return the segmentation probability for each patch (reducing over all the voxels in a patch).\n",
    "    \"\"\"\n",
    "    # old method that just picks the segmentation at the centre of the patch:\n",
    "    # slice near the (slightly off-centre) centre of the patch to choose the class\n",
    "    # patch_gt = mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side]\n",
    "\n",
    "    # combine segmentation results (total counts for all classes across patch) then normalize;\n",
    "    # by summing accross the patch, the segmentation labels can be converted to probabilities to account for \n",
    "    # partial volume effects (if half the patch is one segment and half is another, each will get 50% probability)\n",
    "    # mask (6, 4, 24, 24, 24) -> (6, 4, 3, 8, 3, 8, 3, 8) then sum across the patch dims\n",
    "    patch_gt = mask_block.reshape(-1, brats_classes, block_side // patch_side, patch_side, block_side // patch_side, patch_side, block_side // patch_side, patch_side).sum(axis=-1).sum(axis=-2).sum(axis=-3)\n",
    "    return patch_gt / patch_side ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59440011-8944-464a-b68b-b8f8dc9f44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=train_file, infinite=False, resample_size=resampled_shape):\n",
    "        # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "        img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "        img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(img_gpu)\n",
    "        predictions = torch.softmax(output, axis=1)\n",
    "        # output shape is (8, 4, 240, 240, 155), one segment for each voxel\n",
    "\n",
    "        # current_loss = loss(output, patch_gt.argmax(axis=1))\n",
    "        current_loss = focal_loss(predictions, mask_gpu) # + dice_loss(predictions, patch_gt)\n",
    "                    \n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += current_loss.item()\n",
    "        count += 1\n",
    "        print(f'Training batch loss: {running_loss / count}')\n",
    "        writer.add_scalar('Training batch loss', running_loss / count)\n",
    "        writer.flush()\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88969187-ea0c-4a0f-a096-66d1978806a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():      \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        running_iou = 0\n",
    "        count = 0\n",
    "        for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=test_file, infinite=False, resample_size=resampled_shape):\n",
    "            # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "            img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "            img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "            \n",
    "            output = model(img_gpu)\n",
    "            predictions = torch.softmax(output, axis=1)\n",
    "                        \n",
    "            # current_loss = loss(output, patch_gt.argmax(axis=1))\n",
    "            current_loss = focal_loss(predictions, mask_gpu) # + dice_loss(predictions, patch_gt)\n",
    "                        \n",
    "            running_iou += iou(predictions, mask_gpu)\n",
    "            running_loss += current_loss.item()\n",
    "            count += 1\n",
    "    return running_loss / count, running_iou / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f7209-ba01-4f19-83e3-962e2aa6bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/unetr/trainer_{}'.format(timestamp))\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train()\n",
    "    val_loss, val_iou = validate()\n",
    "    \n",
    "    print(f'Epoch {epoch}: LOSS train {train_loss}; validation {val_loss}; validation IOU {val_iou}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss, 'IOU': val_iou}, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = output_dir + f'model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4dfc-8571-4030-bec2-b66571247f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151e80b-4528-4040-ab61-3d0c21742600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eece571f]",
   "language": "python",
   "name": "conda-env-eece571f-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
