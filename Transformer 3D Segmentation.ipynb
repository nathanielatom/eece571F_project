{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde958e-cf32-480f-b2d8-1e2a79baf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from self_attention_cv.Transformer3Dsegmentation.tranf3Dseg import Transformer3dSeg\n",
    "from self_attention_cv.transunet import TransUnet\n",
    "from self_attention_cv import TransformerEncoder\n",
    "from self_attention_cv import ViT, ResNet50ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e10e54-35c4-4978-99ce-63f923096f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype(np.int16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0b58f-7bb8-4d30-981a-604aa85af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brats_batch(prefix, \n",
    "                         contrasts, \n",
    "                         batch_size=32, \n",
    "                         tumour='*', \n",
    "                         patient_ids='*',\n",
    "                         augment_size=None,\n",
    "                         infinite=True):\n",
    "    \"\"\"\n",
    "    Generate arrays for each batch, for x (data) and y (labels), where the contrast is treated like a colour channel.\n",
    "    \n",
    "    Example:\n",
    "    x_batch shape: (32, 240, 240, 155, 4)\n",
    "    y_batch shape: (32, 240, 240, 155)\n",
    "    \n",
    "    augment_size must be less than or equal to the batch_size, if None will not augment.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_pattern = '{prefix}/MICCAI_BraTS_2018_Data_Training/{tumour}/{patient_id}/{patient_id}_{contrast}.nii.gz'\n",
    "    while True:\n",
    "        n_classes = 4\n",
    "\n",
    "        # get list of filenames for every contrast available\n",
    "        keys = dict(prefix=prefix, tumour=tumour)\n",
    "        filenames_by_contrast = {}\n",
    "        for contrast in contrasts:\n",
    "            filenames_by_contrast[contrast] = glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_ids, **keys)) if patient_ids == '*' else []\n",
    "            if patient_ids != '*':\n",
    "                contrast_files = []\n",
    "                for patient_id in patient_ids:\n",
    "                    contrast_files.extend(glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_id, **keys)))\n",
    "                filenames_by_contrast[contrast] = contrast_files\n",
    "\n",
    "        # get the shape of one 3D volume and initialize the batch lists\n",
    "        arbitrary_contrast = contrasts[0]\n",
    "        shape = nib.load(filenames_by_contrast[arbitrary_contrast][0]).get_fdata().shape\n",
    "\n",
    "        # initialize empty array of batches\n",
    "        x_batch = np.empty((batch_size, ) + shape + (len(contrasts), )) #, dtype=np.int32)\n",
    "        y_batch = np.empty((batch_size, ) + shape + (n_classes,)) #, dtype=np.int32)\n",
    "        num_images = len(filenames_by_contrast[arbitrary_contrast])\n",
    "        np.random.shuffle(filenames_by_contrast[arbitrary_contrast])\n",
    "        for bindex in trange(0, num_images, batch_size):\n",
    "            filenames = filenames_by_contrast[arbitrary_contrast][bindex:bindex + batch_size]\n",
    "            for findex, filename in enumerate(filenames):\n",
    "                for cindex, contrast in enumerate(contrasts):\n",
    "\n",
    "                    # load raw image batches and normalize the pixels\n",
    "                    tmp_img = nib.load(filename.replace(arbitrary_contrast, contrast)).get_fdata()\n",
    "                    tmp_img = scalar.fit_transform(tmp_img.reshape(-1, tmp_img.shape[-1])).reshape(tmp_img.shape)\n",
    "                    x_batch[findex, ..., cindex] = tmp_img\n",
    "\n",
    "                    # load mask batches and change to categorical\n",
    "                    tmp_mask = nib.load(filename.replace(arbitrary_contrast, 'seg')).get_fdata()\n",
    "                    tmp_mask[tmp_mask==4] = 3\n",
    "                    tmp_mask = tmp_mask\n",
    "                    tmp_mask = to_categorical(tmp_mask, num_classes = 4)\n",
    "                    y_batch[findex] = tmp_mask\n",
    "\n",
    "            if bindex + batch_size > num_images:\n",
    "                x_batch, y_batch = x_batch[:num_images - bindex], y_batch[:num_images - bindex]\n",
    "            if augment_size is not None:\n",
    "                # x_aug, y_aug = augment(x_batch, y_batch, augment_size)\n",
    "                x_aug = None\n",
    "                y_aug = None\n",
    "                yield np.append(x_batch, x_aug), np.append(y_batch, y_aug)\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "        if not infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaf9c9-f593-4d08-b620-deb8221256a3",
   "metadata": {},
   "source": [
    "Model Architecture Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c437d0-2cfa-454f-a790-0dde5a8e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/atom/Documents/datasets/brats' # Adam's Station\n",
    "output_dir = prefix + '/transformer_models/'\n",
    "batch_size = 16\n",
    "contrasts = ['t1ce', 'flair', 't2', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213fd10-539d-4aaa-9dff-951031f0ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_classes = 4\n",
    "brats_contrasts = 4\n",
    "brats_x = 240\n",
    "brats_y = 240\n",
    "brats_z = 155\n",
    "\n",
    "block_side = 24 # W in the paper\n",
    "patch_side = 8 # w in the paper, so n = W/w = 3, N = 27\n",
    "embedding_size = 1024 # D\n",
    "transformer_blocks = 5 # K\n",
    "msa_heads = 4\n",
    "mlp_size = 1024\n",
    "\n",
    "dropout = 0.25\n",
    "max_epochs = 50\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476d812-9e5f-443c-a879-efe1c46b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1ea11-8bb7-467e-8791-7cb9b11a9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer3dSeg(subvol_dim=block_side, \n",
    "                         patch_dim=patch_side, \n",
    "                         num_classes=brats_classes,\n",
    "                         in_channels=brats_contrasts,\n",
    "                         dim=embedding_size,\n",
    "                         blocks=transformer_blocks, \n",
    "                         heads=msa_heads, \n",
    "                         dim_linear_block=mlp_size,\n",
    "                         dropout=dropout) #, transformer=TransformerEncoder)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227263a-a52c-4368-9b57-07a2cd885b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906d198-0579-4d3a-b989-97356a3eac1f",
   "metadata": {},
   "source": [
    "Training and Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1b39f-c678-42f7-9296-f344bf13a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "brats_dir = '/MICCAI_BraTS_2018_Data_Training/'\n",
    "\n",
    "data_list_LGG = os.listdir(os.path.join(prefix+brats_dir,'LGG'))\n",
    "data_list_HGG = os.listdir(os.path.join(prefix+brats_dir,'HGG'))\n",
    "dataset_file_list = data_list_HGG + data_list_LGG\n",
    "\n",
    "# shuffle and split the dataset file list\n",
    "random.seed(42)\n",
    "file_list_shuffled = dataset_file_list.copy()\n",
    "random.shuffle(file_list_shuffled)\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_file, test_file = file_list_shuffled[0:int(len(file_list_shuffled)*(1-test_ratio))], file_list_shuffled[int(len(file_list_shuffled)*(1-test_ratio)):]\n",
    "\n",
    "while '.DS_Store' in train_file:\n",
    "    train_file.remove('.DS_Store')\n",
    "while '.DS_Store' in test_file:\n",
    "    test_file.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59440011-8944-464a-b68b-b8f8dc9f44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=train_file, infinite=False):\n",
    "        # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "        img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "        img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "        for i in range(0, brats_x, block_side):\n",
    "            for j in range(0, brats_y, block_side):\n",
    "                for k in range(6, brats_z - block_side, block_side):\n",
    "                    img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(img_block)\n",
    "                    # TODO: get max (worst case) segment class within each patch, not just centre\n",
    "                    current_loss = loss(output, mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side].argmax(axis=1))\n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()\n",
    "                    count += 1\n",
    "        print(f'Training batch loss: {running_loss / count}')\n",
    "        writer.add_scalar('Training batch loss', running_loss / count)\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88969187-ea0c-4a0f-a096-66d1978806a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():      \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        count = 0\n",
    "        for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=test_file, infinite=False):\n",
    "            # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "            img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "            img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "            for i in range(0, brats_x, block_side):\n",
    "                for j in range(0, brats_y, block_side):\n",
    "                    for k in range(6, brats_z - block_side, block_side):\n",
    "                        img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        output = model(img_block)\n",
    "                        current_loss = loss(output, mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side].argmax(axis=1))\n",
    "                        running_loss += current_loss.item()\n",
    "                        count += 1\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f7209-ba01-4f19-83e3-962e2aa6bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    print(f'Epoch {epoch}: LOSS train {train_loss}; validation {val_loss}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss}, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = output_dir + f'model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4dfc-8571-4030-bec2-b66571247f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
