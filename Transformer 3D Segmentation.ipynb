{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edde958e-cf32-480f-b2d8-1e2a79baf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from self_attention_cv.Transformer3Dsegmentation.tranf3Dseg import Transformer3dSeg\n",
    "from self_attention_cv.transunet import TransUnet\n",
    "from self_attention_cv import TransformerEncoder\n",
    "from self_attention_cv import ViT, ResNet50ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624650bb-cc6a-413a-ba42-4083109d4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs: torch.Tensor, labels: torch.Tensor, smooth=1e-6):\n",
    "    # multiclass IOU: intersection is where classes agree; union is any non-null class\n",
    "    \n",
    "    # output shape is (8, 4, 3, 3, 3), one segment for each patch\n",
    "    outputs = outputs.argmax(axis=1)\n",
    "    labels = labels.argmax(axis=1)\n",
    "    \n",
    "    intersection = (outputs == labels).float().sum((-1, -2, -3)) # Will be zero if Truth=0 or Prediction=0\n",
    "    union = ((outputs != 0) | (labels != 0)).float().sum((-1, -2, -3)) # Will be zero if both are 0\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth) # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    return thresholded.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699ef12d-22d0-424d-bb99-c3106853f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1e-6):\n",
    "        # flatten label and prediction tensors\n",
    "        inputs = torch.flatten(inputs)\n",
    "        targets = torch.flatten(targets)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2 * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1f3815-8ff6-493b-9b18-e1608e1cf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.modules.loss._WeightedLoss):\n",
    "    \"\"\"\n",
    "    From https://github.com/ZFTurbo/segmentation_models_3D/blob/cc9f4fdd22387cc1556c77a85c7bea43e541ef1d/segmentation_models_3D/base/functional.py#L259\n",
    "    as oppose to https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py#L11\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, gamma=2, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.weight = weight # weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, pr, gt, smooth=1e-6):\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pr = torch.clamp(pr, smooth, 1 - smooth)\n",
    "        \n",
    "        focal_loss = -1 * gt * torch.log(pr) * (self.alpha * (1 - pr) ** self.gamma)\n",
    "        \n",
    "        # ce_loss = torch.nn.functional.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        # pt = torch.exp(-ce_loss)\n",
    "        # focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        \n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e10e54-35c4-4978-99ce-63f923096f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype(np.int16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e0b58f-7bb8-4d30-981a-604aa85af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brats_batch(prefix, \n",
    "                         contrasts, \n",
    "                         batch_size=32, \n",
    "                         tumour='*', \n",
    "                         patient_ids='*',\n",
    "                         augment_size=None,\n",
    "                         infinite=True):\n",
    "    \"\"\"\n",
    "    Generate arrays for each batch, for x (data) and y (labels), where the contrast is treated like a colour channel.\n",
    "    \n",
    "    Example:\n",
    "    x_batch shape: (32, 240, 240, 155, 4)\n",
    "    y_batch shape: (32, 240, 240, 155)\n",
    "    \n",
    "    augment_size must be less than or equal to the batch_size, if None will not augment.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_pattern = '{prefix}/MICCAI_BraTS_2018_Data_Training/{tumour}/{patient_id}/{patient_id}_{contrast}.nii.gz'\n",
    "    while True:\n",
    "        n_classes = 4\n",
    "\n",
    "        # get list of filenames for every contrast available\n",
    "        keys = dict(prefix=prefix, tumour=tumour)\n",
    "        filenames_by_contrast = {}\n",
    "        for contrast in contrasts:\n",
    "            filenames_by_contrast[contrast] = glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_ids, **keys)) if patient_ids == '*' else []\n",
    "            if patient_ids != '*':\n",
    "                contrast_files = []\n",
    "                for patient_id in patient_ids:\n",
    "                    contrast_files.extend(glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_id, **keys)))\n",
    "                filenames_by_contrast[contrast] = contrast_files\n",
    "\n",
    "        # get the shape of one 3D volume and initialize the batch lists\n",
    "        arbitrary_contrast = contrasts[0]\n",
    "        shape = nib.load(filenames_by_contrast[arbitrary_contrast][0]).get_fdata().shape\n",
    "\n",
    "        # initialize empty array of batches\n",
    "        x_batch = np.empty((batch_size, ) + shape + (len(contrasts), )) #, dtype=np.int32)\n",
    "        y_batch = np.empty((batch_size, ) + shape + (n_classes,)) #, dtype=np.int32)\n",
    "        num_images = len(filenames_by_contrast[arbitrary_contrast])\n",
    "        np.random.shuffle(filenames_by_contrast[arbitrary_contrast])\n",
    "        for bindex in trange(0, num_images, batch_size):\n",
    "            filenames = filenames_by_contrast[arbitrary_contrast][bindex:bindex + batch_size]\n",
    "            for findex, filename in enumerate(filenames):\n",
    "                for cindex, contrast in enumerate(contrasts):\n",
    "\n",
    "                    # load raw image batches and normalize the pixels\n",
    "                    tmp_img = nib.load(filename.replace(arbitrary_contrast, contrast)).get_fdata()\n",
    "                    tmp_img = scalar.fit_transform(tmp_img.reshape(-1, tmp_img.shape[-1])).reshape(tmp_img.shape)\n",
    "                    x_batch[findex, ..., cindex] = tmp_img\n",
    "\n",
    "                    # load mask batches and change to categorical\n",
    "                    tmp_mask = nib.load(filename.replace(arbitrary_contrast, 'seg')).get_fdata()\n",
    "                    tmp_mask[tmp_mask==4] = 3\n",
    "                    tmp_mask = to_categorical(tmp_mask, num_classes=4)\n",
    "                    y_batch[findex] = tmp_mask\n",
    "\n",
    "            if bindex + batch_size > num_images:\n",
    "                x_batch, y_batch = x_batch[:num_images - bindex], y_batch[:num_images - bindex]\n",
    "            if augment_size is not None:\n",
    "                # x_aug, y_aug = augment(x_batch, y_batch, augment_size)\n",
    "                x_aug = None\n",
    "                y_aug = None\n",
    "                yield np.append(x_batch, x_aug), np.append(y_batch, y_aug)\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "        if not infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaf9c9-f593-4d08-b620-deb8221256a3",
   "metadata": {},
   "source": [
    "Model Architecture Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c437d0-2cfa-454f-a790-0dde5a8e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/atom/Documents/datasets/brats' # Adam's Station\n",
    "output_dir = prefix + '/transformer_models/'\n",
    "batch_size = 4 # 16\n",
    "contrasts = ['t1ce', 'flair', 't2', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0213fd10-539d-4aaa-9dff-951031f0ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_classes = 4\n",
    "brats_contrasts = 4\n",
    "brats_x = 240\n",
    "brats_y = 240\n",
    "brats_z = 155\n",
    "\n",
    "block_side = 48 # 24 # W in the paper\n",
    "patch_side = 16 # 8 # w in the paper, so n = W/w = 3, N = 27\n",
    "embedding_size = 2048 # 1024 # D\n",
    "transformer_blocks = 5 # K\n",
    "msa_heads = 5\n",
    "mlp_size = 2048 # 1024\n",
    "\n",
    "dropout = 0.15\n",
    "max_epochs = 50\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e476d812-9e5f-443c-a879-efe1c46b7fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c1ea11-8bb7-467e-8791-7cb9b11a9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer3dSeg(subvol_dim=block_side, \n",
    "                         patch_dim=patch_side, \n",
    "                         num_classes=brats_classes,\n",
    "                         in_channels=brats_contrasts,\n",
    "                         dim=embedding_size,\n",
    "                         blocks=transformer_blocks, \n",
    "                         heads=msa_heads, \n",
    "                         dim_linear_block=mlp_size,\n",
    "                         dropout=dropout) #, transformer=TransformerEncoder)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2227263a-a52c-4368-9b57-07a2cd885b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "focal_loss = FocalLoss()\n",
    "dice_loss = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d663b51d-8558-4c4e-9e0a-9b859adbe851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer3dSeg(\n",
       "  (project_patches): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "  (emb_dropout): Dropout(p=0.15, inplace=False)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (mhsa): MultiHeadSelfAttention(\n",
       "          (to_qvk): Linear(in_features=2048, out_features=6135, bias=False)\n",
       "          (W_0): Linear(in_features=2045, out_features=2048, bias=False)\n",
       "        )\n",
       "        (drop): Dropout(p=0.15, inplace=False)\n",
       "        (norm_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (4): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (mhsa): MultiHeadSelfAttention(\n",
       "          (to_qvk): Linear(in_features=2048, out_features=6135, bias=False)\n",
       "          (W_0): Linear(in_features=2045, out_features=2048, bias=False)\n",
       "        )\n",
       "        (drop): Dropout(p=0.15, inplace=False)\n",
       "        (norm_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (4): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (mhsa): MultiHeadSelfAttention(\n",
       "          (to_qvk): Linear(in_features=2048, out_features=6135, bias=False)\n",
       "          (W_0): Linear(in_features=2045, out_features=2048, bias=False)\n",
       "        )\n",
       "        (drop): Dropout(p=0.15, inplace=False)\n",
       "        (norm_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (4): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (mhsa): MultiHeadSelfAttention(\n",
       "          (to_qvk): Linear(in_features=2048, out_features=6135, bias=False)\n",
       "          (W_0): Linear(in_features=2045, out_features=2048, bias=False)\n",
       "        )\n",
       "        (drop): Dropout(p=0.15, inplace=False)\n",
       "        (norm_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (4): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_seg_head): Linear(in_features=2048, out_features=108, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906d198-0579-4d3a-b989-97356a3eac1f",
   "metadata": {},
   "source": [
    "Training and Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dec1b39f-c678-42f7-9296-f344bf13a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "brats_dir = '/MICCAI_BraTS_2018_Data_Training/'\n",
    "\n",
    "data_list_LGG = os.listdir(os.path.join(prefix+brats_dir,'LGG'))\n",
    "data_list_HGG = os.listdir(os.path.join(prefix+brats_dir,'HGG'))\n",
    "dataset_file_list = data_list_HGG + data_list_LGG\n",
    "\n",
    "# shuffle and split the dataset file list\n",
    "random.seed(42)\n",
    "file_list_shuffled = dataset_file_list.copy()\n",
    "random.shuffle(file_list_shuffled)\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_file, test_file = file_list_shuffled[0:int(len(file_list_shuffled)*(1-test_ratio))], file_list_shuffled[int(len(file_list_shuffled)*(1-test_ratio)):]\n",
    "\n",
    "while '.DS_Store' in train_file:\n",
    "    train_file.remove('.DS_Store')\n",
    "while '.DS_Store' in test_file:\n",
    "    test_file.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066dda77-b81e-4f6a-a53e-b44b128e6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_segmentation(mask_block):\n",
    "    \"\"\"\n",
    "    For a given block, return the segmentation probability for each patch (reducing over all the voxels in a patch).\n",
    "    \"\"\"\n",
    "    # old method that just picks the segmentation at the centre of the patch:\n",
    "    # slice near the (slightly off-centre) centre of the patch to choose the class\n",
    "    # patch_gt = mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side]\n",
    "\n",
    "    # combine segmentation results (total counts for all classes across patch) then normalize;\n",
    "    # by summing accross the patch, the segmentation labels can be converted to probabilities to account for \n",
    "    # partial volume effects (if half the patch is one segment and half is another, each will get 50% probability)\n",
    "    # mask (6, 4, 24, 24, 24) -> (6, 4, 3, 8, 3, 8, 3, 8) then sum across the patch dims\n",
    "    patch_gt = mask_block.reshape(-1, brats_classes, block_side // patch_side, patch_side, block_side // patch_side, patch_side, block_side // patch_side, patch_side).sum(axis=-1).sum(axis=-2).sum(axis=-3)\n",
    "    return patch_gt / patch_side ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59440011-8944-464a-b68b-b8f8dc9f44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=train_file, infinite=False):\n",
    "        # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "        img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "        img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "        for i in range(0, brats_x, block_side):\n",
    "            for j in range(0, brats_y, block_side):\n",
    "                for k in range(6, brats_z - block_side, block_side):\n",
    "                    img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(img_block)\n",
    "                    predictions = torch.softmax(output, axis=1)\n",
    "                    # output shape is (8, 4, 3, 3, 3), one segment for each patch\n",
    "                    patch_gt = ground_truth_segmentation(mask_block)\n",
    "\n",
    "                    # current_loss = loss(output, patch_gt.argmax(axis=1))\n",
    "                    current_loss = focal_loss(predictions, patch_gt) + dice_loss(predictions, patch_gt)\n",
    "                    \n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()\n",
    "                    count += 1\n",
    "        print(f'Training batch loss: {running_loss / count}')\n",
    "        writer.add_scalar('Training batch loss', running_loss / count)\n",
    "        writer.flush()\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88969187-ea0c-4a0f-a096-66d1978806a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():      \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        running_iou = 0\n",
    "        count = 0\n",
    "        for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=test_file, infinite=False):\n",
    "            # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "            img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "            img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "            for i in range(0, brats_x, block_side):\n",
    "                for j in range(0, brats_y, block_side):\n",
    "                    for k in range(6, brats_z - block_side, block_side):\n",
    "                        img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        output = model(img_block)\n",
    "                        predictions = torch.softmax(output, axis=1)\n",
    "                        # output shape is (8, 4, 3, 3, 3), one segment for each patch\n",
    "                        patch_gt = ground_truth_segmentation(mask_block)\n",
    "                        \n",
    "                        # current_loss = loss(output, patch_gt.argmax(axis=1))\n",
    "                        current_loss = focal_loss(predictions, patch_gt) + dice_loss(predictions, patch_gt)\n",
    "                        \n",
    "                        running_iou += iou(predictions, patch_gt)\n",
    "                        running_loss += current_loss.item()\n",
    "                        count += 1\n",
    "    return running_loss / count, running_iou / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "797f7209-ba01-4f19-83e3-962e2aa6bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                            | 1/57 [00:10<10:06, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.04230336864134491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▎                                                                                          | 2/57 [00:21<09:37, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.02533729174276232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▉                                                                                         | 3/57 [00:31<09:18, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.026043848684477983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▌                                                                                       | 4/57 [00:41<09:05, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.02713973172033699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▏                                                                                     | 5/57 [00:51<08:57, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.025116145273985847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████▉                                                                                    | 6/57 [01:02<08:47, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.02616037720295733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████▌                                                                                  | 7/57 [01:12<08:36, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.02482804278501135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████████▏                                                                                | 8/57 [01:22<08:25, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.02390835884261681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████▊                                                                               | 9/57 [01:33<08:15, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 0.023252398289832516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████▊                                                                               | 9/57 [01:36<08:36, 10.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m      7\u001b[0m     val_loss, val_iou \u001b[38;5;241m=\u001b[39m validate()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: LOSS train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; validation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; validation IOU \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_iou\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, mask \u001b[38;5;129;01min\u001b[39;00m generate_brats_batch(prefix, contrasts, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, patient_ids\u001b[38;5;241m=\u001b[39mtrain_file, infinite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     img, mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(img, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mrollaxis(mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     img_gpu, mask_gpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(img)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mFloatTensor(mask)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mgenerate_brats_batch\u001b[0;34m(prefix, contrasts, batch_size, tumour, patient_ids, augment_size, infinite)\u001b[0m\n\u001b[1;32m     48\u001b[0m tmp_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(filename\u001b[38;5;241m.\u001b[39mreplace(arbitrary_contrast, contrast))\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m     49\u001b[0m tmp_img \u001b[38;5;241m=\u001b[39m scalar\u001b[38;5;241m.\u001b[39mfit_transform(tmp_img\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tmp_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(tmp_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 50\u001b[0m x_batch[findex, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, cindex] \u001b[38;5;241m=\u001b[39m tmp_img\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# load mask batches and change to categorical\u001b[39;00m\n\u001b[1;32m     53\u001b[0m tmp_mask \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(filename\u001b[38;5;241m.\u001b[39mreplace(arbitrary_contrast, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mget_fdata()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train()\n",
    "    val_loss, val_iou = validate()\n",
    "    \n",
    "    print(f'Epoch {epoch}: LOSS train {train_loss}; validation {val_loss}; validation IOU {val_iou}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss, 'IOU': val_iou}, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = output_dir + f'model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4dfc-8571-4030-bec2-b66571247f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
