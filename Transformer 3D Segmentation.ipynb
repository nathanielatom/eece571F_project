{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edde958e-cf32-480f-b2d8-1e2a79baf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from self_attention_cv.Transformer3Dsegmentation.tranf3Dseg import Transformer3dSeg\n",
    "from self_attention_cv.transunet import TransUnet\n",
    "from self_attention_cv import TransformerEncoder\n",
    "from self_attention_cv import ViT, ResNet50ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699ef12d-22d0-424d-bb99-c3106853f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):        \n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        # flatten label and prediction tensors\n",
    "        # inputs = inputs.view(-1)\n",
    "        # targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1f3815-8ff6-493b-9b18-e1608e1cf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, reduction='sum'):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight # weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).sum()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e10e54-35c4-4978-99ce-63f923096f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype(np.int16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e0b58f-7bb8-4d30-981a-604aa85af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brats_batch(prefix, \n",
    "                         contrasts, \n",
    "                         batch_size=32, \n",
    "                         tumour='*', \n",
    "                         patient_ids='*',\n",
    "                         augment_size=None,\n",
    "                         infinite=True):\n",
    "    \"\"\"\n",
    "    Generate arrays for each batch, for x (data) and y (labels), where the contrast is treated like a colour channel.\n",
    "    \n",
    "    Example:\n",
    "    x_batch shape: (32, 240, 240, 155, 4)\n",
    "    y_batch shape: (32, 240, 240, 155)\n",
    "    \n",
    "    augment_size must be less than or equal to the batch_size, if None will not augment.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_pattern = '{prefix}/MICCAI_BraTS_2018_Data_Training/{tumour}/{patient_id}/{patient_id}_{contrast}.nii.gz'\n",
    "    while True:\n",
    "        n_classes = 4\n",
    "\n",
    "        # get list of filenames for every contrast available\n",
    "        keys = dict(prefix=prefix, tumour=tumour)\n",
    "        filenames_by_contrast = {}\n",
    "        for contrast in contrasts:\n",
    "            filenames_by_contrast[contrast] = glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_ids, **keys)) if patient_ids == '*' else []\n",
    "            if patient_ids != '*':\n",
    "                contrast_files = []\n",
    "                for patient_id in patient_ids:\n",
    "                    contrast_files.extend(glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_id, **keys)))\n",
    "                filenames_by_contrast[contrast] = contrast_files\n",
    "\n",
    "        # get the shape of one 3D volume and initialize the batch lists\n",
    "        arbitrary_contrast = contrasts[0]\n",
    "        shape = nib.load(filenames_by_contrast[arbitrary_contrast][0]).get_fdata().shape\n",
    "\n",
    "        # initialize empty array of batches\n",
    "        x_batch = np.empty((batch_size, ) + shape + (len(contrasts), )) #, dtype=np.int32)\n",
    "        y_batch = np.empty((batch_size, ) + shape + (n_classes,)) #, dtype=np.int32)\n",
    "        num_images = len(filenames_by_contrast[arbitrary_contrast])\n",
    "        np.random.shuffle(filenames_by_contrast[arbitrary_contrast])\n",
    "        for bindex in trange(0, num_images, batch_size):\n",
    "            filenames = filenames_by_contrast[arbitrary_contrast][bindex:bindex + batch_size]\n",
    "            for findex, filename in enumerate(filenames):\n",
    "                for cindex, contrast in enumerate(contrasts):\n",
    "\n",
    "                    # load raw image batches and normalize the pixels\n",
    "                    tmp_img = nib.load(filename.replace(arbitrary_contrast, contrast)).get_fdata()\n",
    "                    tmp_img = scalar.fit_transform(tmp_img.reshape(-1, tmp_img.shape[-1])).reshape(tmp_img.shape)\n",
    "                    x_batch[findex, ..., cindex] = tmp_img\n",
    "\n",
    "                    # load mask batches and change to categorical\n",
    "                    tmp_mask = nib.load(filename.replace(arbitrary_contrast, 'seg')).get_fdata()\n",
    "                    tmp_mask[tmp_mask==4] = 3\n",
    "                    tmp_mask = to_categorical(tmp_mask, num_classes = 4)\n",
    "                    y_batch[findex] = tmp_mask\n",
    "\n",
    "            if bindex + batch_size > num_images:\n",
    "                x_batch, y_batch = x_batch[:num_images - bindex], y_batch[:num_images - bindex]\n",
    "            if augment_size is not None:\n",
    "                # x_aug, y_aug = augment(x_batch, y_batch, augment_size)\n",
    "                x_aug = None\n",
    "                y_aug = None\n",
    "                yield np.append(x_batch, x_aug), np.append(y_batch, y_aug)\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "        if not infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaf9c9-f593-4d08-b620-deb8221256a3",
   "metadata": {},
   "source": [
    "Model Architecture Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c437d0-2cfa-454f-a790-0dde5a8e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/atom/Documents/datasets/brats' # Adam's Station\n",
    "output_dir = prefix + '/transformer_models/'\n",
    "batch_size = 16\n",
    "contrasts = ['t1ce', 'flair', 't2', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0213fd10-539d-4aaa-9dff-951031f0ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_classes = 4\n",
    "brats_contrasts = 4\n",
    "brats_x = 240\n",
    "brats_y = 240\n",
    "brats_z = 155\n",
    "\n",
    "block_side = 24 # W in the paper\n",
    "patch_side = 8 # w in the paper, so n = W/w = 3, N = 27\n",
    "embedding_size = 1024 # D\n",
    "transformer_blocks = 5 # K\n",
    "msa_heads = 4\n",
    "mlp_size = 1024\n",
    "\n",
    "dropout = 0.15\n",
    "max_epochs = 50\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e476d812-9e5f-443c-a879-efe1c46b7fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c1ea11-8bb7-467e-8791-7cb9b11a9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer3dSeg(subvol_dim=block_side, \n",
    "                         patch_dim=patch_side, \n",
    "                         num_classes=brats_classes,\n",
    "                         in_channels=brats_contrasts,\n",
    "                         dim=embedding_size,\n",
    "                         blocks=transformer_blocks, \n",
    "                         heads=msa_heads, \n",
    "                         dim_linear_block=mlp_size,\n",
    "                         dropout=dropout) #, transformer=TransformerEncoder)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2227263a-a52c-4368-9b57-07a2cd885b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "focal_loss = FocalLoss(gamma=2) #, alpha=0.25, size_average=False)\n",
    "dice_loss = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906d198-0579-4d3a-b989-97356a3eac1f",
   "metadata": {},
   "source": [
    "Training and Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec1b39f-c678-42f7-9296-f344bf13a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "\n",
    "brats_dir = '/MICCAI_BraTS_2018_Data_Training/'\n",
    "\n",
    "data_list_LGG = os.listdir(os.path.join(prefix+brats_dir,'LGG'))\n",
    "data_list_HGG = os.listdir(os.path.join(prefix+brats_dir,'HGG'))\n",
    "dataset_file_list = data_list_HGG + data_list_LGG\n",
    "\n",
    "# shuffle and split the dataset file list\n",
    "random.seed(42)\n",
    "file_list_shuffled = dataset_file_list.copy()\n",
    "random.shuffle(file_list_shuffled)\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_file, test_file = file_list_shuffled[0:int(len(file_list_shuffled)*(1-test_ratio))], file_list_shuffled[int(len(file_list_shuffled)*(1-test_ratio)):]\n",
    "\n",
    "while '.DS_Store' in train_file:\n",
    "    train_file.remove('.DS_Store')\n",
    "while '.DS_Store' in test_file:\n",
    "    test_file.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59440011-8944-464a-b68b-b8f8dc9f44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=train_file, infinite=False):\n",
    "        # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "        img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "        img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "        for i in range(0, brats_x, block_side):\n",
    "            for j in range(0, brats_y, block_side):\n",
    "                for k in range(6, brats_z - block_side, block_side):\n",
    "                    img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(img_block)\n",
    "                    \n",
    "                    # slice near the (slightly off-centre) centre of the patch to choose the class\n",
    "                    # patch_mask = mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side]\n",
    "                    \n",
    "                    # combined segmentation results (total counts for all classes across patch)\n",
    "                    patch_mask = mask_block.reshape(-1, brats_classes, block_side // patch_side, patch_side, block_side // patch_side, patch_side, block_side // patch_side, patch_side).sum(axis=-1).sum(axis=-2).sum(axis=-3) / 2 ** (3 * block_side // patch_side)\n",
    "                    # patch_mask = torch.round(patch_mask).to(torch.int64)\n",
    "                    \n",
    "                    # current_loss = loss(output, patch_mask.argmax(axis=1))\n",
    "                    current_loss = focal_loss(output, patch_mask) + dice_loss(output, patch_mask)\n",
    "                    \n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()\n",
    "                    count += 1\n",
    "        print(f'Training batch loss: {running_loss / count}')\n",
    "        writer.add_scalar('Training batch loss', running_loss / count)\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88969187-ea0c-4a0f-a096-66d1978806a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():      \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0\n",
    "        count = 0\n",
    "        for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size, patient_ids=test_file, infinite=False):\n",
    "            # img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\n",
    "            img, mask = np.rollaxis(img, -1, 1), np.rollaxis(mask, -1, 1)\n",
    "            img_gpu, mask_gpu = torch.FloatTensor(img).to(device), torch.FloatTensor(mask).to(device)\n",
    "            for i in range(0, brats_x, block_side):\n",
    "                for j in range(0, brats_y, block_side):\n",
    "                    for k in range(6, brats_z - block_side, block_side):\n",
    "                        img_block = img_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        mask_block = mask_gpu[..., i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        output = model(img_block)\n",
    "\n",
    "                        # slice near the (slightly off-centre) centre of the patch to choose the class\n",
    "                        # patch_mask = mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side]\n",
    "\n",
    "                        # combined segmentation results (total counts for all classes across patch)\n",
    "                        patch_mask = mask_block.reshape(-1, brats_classes, block_side // patch_side, patch_side, block_side // patch_side, patch_side, block_side // patch_side, patch_side).sum(axis=-1).sum(axis=-2).sum(axis=-3) / 2 ** (3 * block_side // patch_side)\n",
    "                        # patch_mask = torch.round(patch_mask).to(torch.int64)\n",
    "                        \n",
    "                        # current_loss = loss(output, patch_mask.argmax(axis=1))\n",
    "                        current_loss = focal_loss(output, patch_mask) + dice_loss(output, patch_mask)\n",
    "\n",
    "                        running_loss += current_loss.item()\n",
    "                        count += 1\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "797f7209-ba01-4f19-83e3-962e2aa6bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▎                                                                                       | 1/15 [00:40<09:27, 40.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 62.681525887950166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████████▌                                                                                 | 2/15 [01:20<08:41, 40.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 52.45108463408619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▊                                                                           | 3/15 [01:59<07:58, 39.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 45.82528668188102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████████                                                                     | 4/15 [02:39<07:18, 39.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 41.4151082617318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████▎                                                              | 5/15 [03:19<06:38, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 38.72588729434041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████▌                                                        | 6/15 [03:59<05:59, 39.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 37.40398311541222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████████████▊                                                  | 7/15 [04:39<05:19, 39.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 36.239167474737236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████████████▏                                           | 8/15 [05:19<04:40, 40.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 35.816772397969714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████▍                                     | 9/15 [06:00<04:00, 40.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 35.221254653520276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████                               | 10/15 [06:40<03:20, 40.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 35.39871095958856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████▏                        | 11/15 [07:20<02:40, 40.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 34.82974914445379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 12/15 [07:59<01:59, 39.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 34.46639089821611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████████████████▌            | 13/15 [08:39<01:19, 39.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 34.686951077197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████████████████████████████████████████████████████████████████▊      | 14/15 [09:19<00:39, 39.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 34.37614883039152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [09:34<00:00, 38.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 32.60390039670493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:43<00:00, 25.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: LOSS train 32.60390039670493; validation 38.440587958355124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▎                                                                                       | 1/15 [00:40<09:25, 40.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 30.032193303943302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████████▌                                                                                 | 2/15 [01:20<08:42, 40.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.77421239723762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▊                                                                           | 3/15 [02:00<08:02, 40.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.57218762432846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████████                                                                     | 4/15 [02:40<07:22, 40.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 30.917413500943997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████▎                                                              | 5/15 [03:21<06:42, 40.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 30.78353777543828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████▌                                                        | 6/15 [04:01<06:01, 40.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.692633887784645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████████████▊                                                  | 7/15 [04:41<05:20, 40.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.76211533081959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████████████▏                                           | 8/15 [05:21<04:41, 40.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.615609678601395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████▍                                     | 9/15 [06:01<04:01, 40.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 32.2220095830564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████                               | 10/15 [06:41<03:20, 40.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 32.139558829386544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████▏                        | 11/15 [07:21<02:40, 40.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.567465552182878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 12/15 [08:01<02:00, 40.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.464198989812658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████████████████▌            | 13/15 [08:41<01:20, 40.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch loss: 31.262181725951063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████████████████▌            | 13/15 [09:06<01:24, 42.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m      7\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: LOSS train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; validation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, mask \u001b[38;5;129;01min\u001b[39;00m generate_brats_batch(prefix, contrasts, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, patient_ids\u001b[38;5;241m=\u001b[39mtrain_file, infinite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# img (8, 240, 240, 155, 4) -> (8, 4, 240, 240, 155)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     img, mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(img, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mrollaxis(mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     img_gpu, mask_gpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(img)\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, brats_x, block_side):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, brats_y, block_side):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    print(f'Epoch {epoch}: LOSS train {train_loss}; validation {val_loss}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       {'Training' : train_loss, 'Validation' : val_loss}, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_path = output_dir + f'model_{timestamp}_{epoch}'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4dfc-8571-4030-bec2-b66571247f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
