{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edde958e-cf32-480f-b2d8-1e2a79baf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from self_attention_cv.Transformer3Dsegmentation.tranf3Dseg import Transformer3dSeg\n",
    "from self_attention_cv.transunet import TransUnet\n",
    "from self_attention_cv import TransformerEncoder\n",
    "from self_attention_cv import ViT, ResNet50ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e10e54-35c4-4978-99ce-63f923096f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y.astype(np.int16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e0b58f-7bb8-4d30-981a-604aa85af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brats_batch(prefix, \n",
    "                         contrasts, \n",
    "                         batch_size=32, \n",
    "                         tumour='*', \n",
    "                         patient_ids='*',\n",
    "                         augment_size=None,\n",
    "                         infinite=True):\n",
    "    \"\"\"\n",
    "    Generate arrays for each batch, for x (data) and y (labels), where the contrast is treated like a colour channel.\n",
    "    \n",
    "    Example:\n",
    "    x_batch shape: (32, 240, 240, 155, 4)\n",
    "    y_batch shape: (32, 240, 240, 155)\n",
    "    \n",
    "    augment_size must be less than or equal to the batch_size, if None will not augment.\n",
    "    \n",
    "    \"\"\"\n",
    "    file_pattern = '{prefix}/MICCAI_BraTS_2018_Data_Training/{tumour}/{patient_id}/{patient_id}_{contrast}.nii.gz'\n",
    "    while True:\n",
    "        n_classes = 4\n",
    "\n",
    "        # get list of filenames for every contrast available\n",
    "        keys = dict(prefix=prefix, tumour=tumour)\n",
    "        filenames_by_contrast = {}\n",
    "        for contrast in contrasts:\n",
    "            filenames_by_contrast[contrast] = glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_ids, **keys)) if patient_ids == '*' else []\n",
    "            if patient_ids != '*':\n",
    "                contrast_files = []\n",
    "                for patient_id in patient_ids:\n",
    "                    contrast_files.extend(glob.glob(file_pattern.format(contrast=contrast, patient_id=patient_id, **keys)))\n",
    "                filenames_by_contrast[contrast] = contrast_files\n",
    "\n",
    "        # get the shape of one 3D volume and initialize the batch lists\n",
    "        arbitrary_contrast = contrasts[0]\n",
    "        shape = nib.load(filenames_by_contrast[arbitrary_contrast][0]).get_fdata().shape\n",
    "\n",
    "        # initialize empty array of batches\n",
    "        x_batch = np.empty((batch_size, ) + shape + (len(contrasts), )) #, dtype=np.int32)\n",
    "        y_batch = np.empty((batch_size, ) + shape + (n_classes,)) #, dtype=np.int32)\n",
    "        num_images = len(filenames_by_contrast[arbitrary_contrast])\n",
    "        np.random.shuffle(filenames_by_contrast[arbitrary_contrast])\n",
    "        for bindex in tqdm(range(0, num_images, batch_size), total=num_images):\n",
    "            filenames = filenames_by_contrast[arbitrary_contrast][bindex:bindex + batch_size]\n",
    "            for findex, filename in enumerate(filenames):\n",
    "                for cindex, contrast in enumerate(contrasts):\n",
    "\n",
    "                    # load raw image batches and normalize the pixels\n",
    "                    tmp_img = nib.load(filename.replace(arbitrary_contrast, contrast)).get_fdata()\n",
    "                    tmp_img = scalar.fit_transform(tmp_img.reshape(-1, tmp_img.shape[-1])).reshape(tmp_img.shape)\n",
    "                    x_batch[findex, ..., cindex] = tmp_img\n",
    "\n",
    "                    # load mask batches and change to categorical\n",
    "                    tmp_mask = nib.load(filename.replace(arbitrary_contrast, 'seg')).get_fdata()\n",
    "                    tmp_mask[tmp_mask==4] = 3\n",
    "                    tmp_mask = tmp_mask\n",
    "                    tmp_mask = to_categorical(tmp_mask, num_classes = 4)\n",
    "                    y_batch[findex] = tmp_mask\n",
    "\n",
    "            if bindex + batch_size > num_images:\n",
    "                x_batch, y_batch = x_batch[:num_images - bindex], y_batch[:num_images - bindex]\n",
    "            if augment_size is not None:\n",
    "                # x_aug, y_aug = augment(x_batch, y_batch, augment_size)\n",
    "                x_aug = None\n",
    "                y_aug = None\n",
    "                yield np.append(x_batch, x_aug), np.append(y_batch, y_aug)\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "        if not infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaf9c9-f593-4d08-b620-deb8221256a3",
   "metadata": {},
   "source": [
    "Model Architecture Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c437d0-2cfa-454f-a790-0dde5a8e205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/atom/Documents/datasets/brats' # Adam's Station\n",
    "batch_size = 4\n",
    "contrasts = ['t1ce', 'flair', 't2', 't1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0213fd10-539d-4aaa-9dff-951031f0ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_classes = 4\n",
    "brats_contrasts = 4\n",
    "brats_x = 240\n",
    "brats_y = 240\n",
    "brats_z = 155\n",
    "\n",
    "block_side = 24 # W in the paper\n",
    "patch_side = 8 # w in the paper, so n = W/w = 3, N = 27\n",
    "embedding_size = 1024 # D\n",
    "transformer_blocks = 5 # K\n",
    "msa_heads = 3\n",
    "mlp_size = 1024\n",
    "\n",
    "dropout = 0.2\n",
    "max_epochs = 5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e476d812-9e5f-443c-a879-efe1c46b7fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c1ea11-8bb7-467e-8791-7cb9b11a9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer3dSeg(subvol_dim=block_side, \n",
    "                         patch_dim=patch_side, \n",
    "                         num_classes=brats_classes,\n",
    "                         in_channels=brats_contrasts,\n",
    "                         dim=embedding_size,\n",
    "                         blocks=transformer_blocks, \n",
    "                         heads=msa_heads, \n",
    "                         dim_linear_block=mlp_size,\n",
    "                         dropout=dropout) #, transformer=TransformerEncoder)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2227263a-a52c-4368-9b57-07a2cd885b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f7209-ba01-4f19-83e3-962e2aa6bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████▏                                                                    | 72/285 [20:03<59:20, 16.72s/it]\n",
      " 25%|███████████████████████▏                                                                    | 72/285 [20:11<59:43, 16.82s/it]\n",
      " 25%|██████████████████████▋                                                                   | 72/285 [20:18<1:00:03, 16.92s/it]\n",
      " 25%|██████████████████████▋                                                                   | 72/285 [20:26<1:00:27, 17.03s/it]\n",
      " 25%|██████████████████████▋                                                                   | 72/285 [20:31<1:00:43, 17.11s/it]\n",
      " 25%|██████████████████████▋                                                                   | 72/285 [20:28<1:00:33, 17.06s/it]\n",
      " 25%|███████████████████████▏                                                                    | 72/285 [19:56<58:58, 16.61s/it]\n",
      "  3%|██▊                                                                                        | 9/285 [02:30<1:17:02, 16.75s/it]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "for epoch in range(max_epochs):\n",
    "    for img, mask in generate_brats_batch(prefix, contrasts, batch_size=batch_size):\n",
    "        # for img, mask in zip(imgs, masks):\n",
    "        for i in range(0, brats_x, block_side):\n",
    "            for j in range(0, brats_y, block_side):\n",
    "                for k in range(6, brats_z - block_side, block_side):\n",
    "                    img_block = img[:, i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                    mask_block = mask[:, i:i+block_side, j:j+block_side, k:k+block_side]\n",
    "                        \n",
    "        # img_block, mask_block = torch.FloatTensor(np.rollaxis(img_block, -1, 1)[..., 6:150]), torch.FloatTensor(np.rollaxis(mask_block, -1, 1)[..., 6:150])\n",
    "                    img_block, mask_block = torch.FloatTensor(np.rollaxis(img_block, -1, 1)), torch.FloatTensor(np.rollaxis(mask_block, -1, 1))\n",
    "                    img_block, mask_block = img_block.to(device), mask_block.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(img_block)\n",
    "                    # TODO: get max (worst case) segment class within each patch, not just centre\n",
    "                    current_loss = loss(output, mask_block[..., patch_side // 2::patch_side, patch_side // 2::patch_side, patch_side // 2::patch_side].argmax(axis=1))\n",
    "                    current_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += current_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4dfc-8571-4030-bec2-b66571247f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
